{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining words from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import array_contains\n",
    "import unicodedata\n",
    "import pycountry\n",
    "import string\n",
    "from operator import add\n",
    "import spacy\n",
    "import os\n",
    "import time\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Analysing Wikipedia\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"./nowiki-20210111-cirrussearch-content.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the schema just to explore the dataset. Found [a description of the JSON dump format on Wikipedia](https://meta.wikimedia.org/wiki/Data_dumps/Misc_dumps_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- auxiliary_text: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- category: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- content_model: string (nullable = true)\n",
      " |-- coordinates: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- coord: struct (nullable = true)\n",
      " |    |    |    |-- lat: double (nullable = true)\n",
      " |    |    |    |-- lon: double (nullable = true)\n",
      " |    |    |-- country: string (nullable = true)\n",
      " |    |    |-- dim: long (nullable = true)\n",
      " |    |    |-- globe: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- primary: boolean (nullable = true)\n",
      " |    |    |-- region: string (nullable = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |-- create_timestamp: string (nullable = true)\n",
      " |-- defaultsort: string (nullable = true)\n",
      " |-- display_title: string (nullable = true)\n",
      " |-- external_link: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- heading: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- incoming_links: long (nullable = true)\n",
      " |-- index: struct (nullable = true)\n",
      " |    |-- _id: string (nullable = true)\n",
      " |    |-- _type: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- namespace: long (nullable = true)\n",
      " |-- namespace_text: string (nullable = true)\n",
      " |-- opening_text: string (nullable = true)\n",
      " |-- ores_articletopic: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- ores_articletopics: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- outgoing_link: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- popularity_score: double (nullable = true)\n",
      " |-- redirect: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- namespace: long (nullable = true)\n",
      " |    |    |-- title: string (nullable = true)\n",
      " |-- score: double (nullable = true)\n",
      " |-- source_text: string (nullable = true)\n",
      " |-- template: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- text_bytes: long (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- version: long (nullable = true)\n",
      " |-- version_type: string (nullable = true)\n",
      " |-- wiki: string (nullable = true)\n",
      " |-- wikibase_item: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------+--------------------+--------------------+----------------+-------------+--------------------+--------------------+--------------+--------------+--------+---------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------+------------+------+-------------+\n",
      "|      auxiliary_text|            category|content_model|         coordinates|    create_timestamp|     defaultsort|display_title|       external_link|             heading|incoming_links|         index|language|namespace|namespace_text|        opening_text|   ores_articletopic|  ores_articletopics|       outgoing_link|    popularity_score|            redirect|             score|         source_text|            template|                text|text_bytes|           timestamp|               title| version|version_type|  wiki|wikibase_item|\n",
      "+--------------------+--------------------+-------------+--------------------+--------------------+----------------+-------------+--------------------+--------------------+--------------+--------------+--------+---------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------+------------+------+-------------+\n",
      "|                null|                null|         null|                null|                null|            null|         null|                null|                null|          null|[796279, page]|    null|     null|          null|                null|                null|                null|                null|                null|                null|              null|                null|                null|                null|      null|                null|                null|    null|        null|  null|         null|\n",
      "|[Hundene i Riga o...|[Artikler med fil...|     wikitext|                  []|2010-06-27T18:30:01Z|           false|         null|[https://www.wiki...|[Handling, Rollel...|            10|          null|      nb|        0|              |Hundene i Riga er...|                null|                null|[Björn_Kjellman, ...|1.186194065204198...|[[0, Hundarna i R...|              null|{{Kursiv tittel}}...|[Mal:Kursiv titte...|Hundene i Riga er...|      3673|2019-04-23T10:14:01Z|Hundene i Riga (f...|19391640|    external|nowiki|    Q10527170|\n",
      "|                null|                null|         null|                null|                null|            null|         null|                null|                null|          null|[296742, page]|    null|     null|          null|                null|                null|                null|                null|                null|                null|              null|                null|                null|                null|      null|                null|                null|    null|        null|  null|         null|\n",
      "|[Kill Buljo Gener...|[Artikler med død...|     wikitext|                  []|2007-03-18T20:54:18Z|           false|         null|[http://film.medi...|[Rolleliste, Anme...|            34|          null|      nb|        0|              |Kill Buljo er en ...|[Culture.Media.Fi...|[Culture.Media.Fi...|[Wikipedia:Stilma...|1.171366639389145...|[[0, Kill buljo],...|              null|{{Kursiv tittel}}...|[Mal:Kursiv titte...|Kill Buljo er en ...|      3521|2020-04-02T11:05:31Z|          Kill Buljo|20332756|    external|nowiki|     Q1741317|\n",
      "|                null|                null|         null|                null|                null|            null|         null|                null|                null|          null|[354927, page]|    null|     null|          null|                null|                null|                null|                null|                null|                null|              null|                null|                null|                null|      null|                null|                null|    null|        null|  null|         null|\n",
      "|[«Bro, bro brille...|[Sangleker, Julel...|     wikitext|                  []|2007-07-25T08:52:41Z|           false|         null|[http://www.rimog...|[Mange navn og ma...|            17|          null|      nb|        0|              |Bro, bro brille e...|                null|                null|[1014, 1957, Alli...|8.260788530815613E-6|[[0, Bro bro bril...|1.1926010434961E-5|<!--[[Fil:Nagasak...|                  []|Bro, bro brille e...|     12266|2019-01-09T10:10:54Z|     Bro, bro brille|19090815|    external|nowiki|      Q468404|\n",
      "|                null|                null|         null|                null|                null|            null|         null|                null|                null|          null|[297525, page]|    null|     null|          null|                null|                null|                null|                null|                null|                null|              null|                null|                null|                null|      null|                null|                null|    null|        null|  null|         null|\n",
      "|[Marcus Furius Ca...|[Sider med refera...|     wikitext|                  []|2007-03-20T12:55:14Z|           false|         null|[https://d-nb.inf...|                  []|             9|          null|      nb|        0|              |Marcus Furius Cam...|[Culture.Biograph...|[Culture.Biograph...|[365_f.Kr., 387_f...|7.727424079410719E-7|[[0, Marcus Furiu...|              null|{{infoboks biogra...|[Mal:Infoboks bio...|Marcus Furius Cam...|      1155|2019-09-18T23:56:53Z|Marcus Furius Cam...|19798658|    external|nowiki|      Q294862|\n",
      "|                null|                null|         null|                null|                null|            null|         null|                null|                null|          null|[357417, page]|    null|     null|          null|                null|                null|                null|                null|                null|                null|              null|                null|                null|                null|      null|                null|                null|    null|        null|  null|         null|\n",
      "|[Final Fantasy II...|[Artikler med spi...|     wikitext|                  []|2007-08-01T13:14:37Z|Final Fantasy 03|         null|[https://www.wiki...|                  []|            40|          null|      nb|        0|              |Final Fantasy III...|[Culture.Media.Vi...|[Culture.Media.Vi...|[Wikipedia:Stilma...|7.080675883556239E-7|        [[0, Ffiii]]|              null|{{Infoboks videos...|[Mal:Infoboks vid...|Final Fantasy III...|      2087|2016-04-11T18:17:25Z|   Final Fantasy III|16188033|    external|nowiki|      Q687559|\n",
      "|                null|                null|         null|                null|                null|            null|         null|                null|                null|          null|[933887, page]|    null|     null|          null|                null|                null|                null|                null|                null|                null|              null|                null|                null|                null|      null|                null|                null|    null|        null|  null|         null|\n",
      "|[Biskoppelig samf...|[Andorransk polit...|     wikitext|                  []|2011-06-29T17:29:43Z|           false|         null|[http://www.copri...|                  []|            71|          null|      nb|        0|              |Liste over samfyr...|[Culture.Biograph...|[Culture.Biograph...|[1278, 1293, 1295...|7.867417648395821E-7|[[0, Samfyrste av...|5.2176295652954E-7|'''Liste over sam...|                  []|Liste over samfyr...|     13272|2018-07-05T23:06:42Z|Liste over samfyr...|18674391|    external|nowiki|    Q16021008|\n",
      "|                null|                null|         null|                null|                null|            null|         null|                null|                null|          null|[193846, page]|    null|     null|          null|                null|                null|                null|                null|                null|                null|              null|                null|                null|                null|      null|                null|                null|    null|        null|  null|         null|\n",
      "|[Kristiansand kat...|[Artikler med død...|     wikitext|[[[58.16, 8.01027...|2006-09-20T14:53:58Z|           false|         null|[http://www.dagbl...|[Historie, Kunsts...|           128|          null|      nb|        0|              |Kristiansand kate...|[Geography.Region...|[Geography.Region...|[Norge, 1492, 164...|3.929267840988906E-6|[[0, Kristiansand...|              null|{{Infoboks skole\n",
      "...|[Mal:Infoboks sko...|Kristiansand kate...|      8272|2020-09-14T10:15:43Z|Kristiansand kate...|20763453|    external|nowiki|     Q1780675|\n",
      "|                null|                null|         null|                null|                null|            null|         null|                null|                null|          null|[495331, page]|    null|     null|          null|                null|                null|                null|                null|                null|                null|              null|                null|                null|                null|      null|                null|                null|    null|        null|  null|         null|\n",
      "|                  []|[Franskmenn, Den ...|     wikitext|                  []|2008-06-08T16:50:11Z|Duault, Isabelle|         null|[http://www.konge...|                  []|             4|          null|      nb|        0|              |Isabelle Duault e...|                null|                null|[Den_Kongelige_No...|2.965485163010495...|                  []|5.2176295652954E-7|'''Isabelle Duaul...|                  []|Isabelle Duault e...|       877|2016-01-07T13:13:09Z|     Isabelle Duault|15423889|    external|nowiki|    Q11977530|\n",
      "|                null|                null|         null|                null|                null|            null|         null|                null|                null|          null|[285838, page]|    null|     null|          null|                null|                null|                null|                null|                null|                null|              null|                null|                null|                null|      null|                null|                null|    null|        null|  null|         null|\n",
      "|[Laget kirke Karl...|[Artikler hvor om...|     wikitext|[[[58.67980999999...|2007-02-24T22:59:15Z|           false|         null|[https://www.wiki...|        [Litteratur]|            25|          null|      nb|        0|              |Laget kirke er en...|                null|[Culture.Philosop...|[Mal:Kirker_i_Aus...|7.867417648395821E-7|[[0, Laget kirkes...|              null|{{Infoboks kirke\n",
      "...|[Mal:Infoboks kir...|Laget kirke er en...|      1032|2020-05-02T04:03:13Z|         Laget kirke|20415198|    external|nowiki|    Q11982942|\n",
      "|                null|                null|         null|                null|                null|            null|         null|                null|                null|          null|[243087, page]|    null|     null|          null|                null|                null|                null|                null|                null|                null|              null|                null|                null|                null|      null|                null|                null|    null|        null|  null|         null|\n",
      "|                  []|[Artikler med off...|     wikitext|                  []|2006-12-10T21:44:38Z|           false|         null|[http://www.ferra...|  [Bilder, Bilder 2]|            10|          null|      nb|        0|              |Enzo Ferrari (200...|[Culture.Sports|3...|[Culture.Sports|3...|[2005, 28._juni, ...|2.668936646709445...| [[0, Ferrari Enzo]]| 5.590317391388E-6|'''Enzo Ferrari''...|[Mal:Wayback, Mal...|Enzo Ferrari (200...|      2719|2020-05-19T19:24:45Z|Enzo Ferrari (bil...|20504649|    external|nowiki|      Q269880|\n",
      "+--------------------+--------------------+-------------+--------------------+--------------------+----------------+-------------+--------------------+--------------------+--------------+--------------+--------+---------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------+------------+------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find columns to filter on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|content_model|\n",
      "+-------------+\n",
      "|         null|\n",
      "|     wikitext|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Should one filter out null?\n",
    "df.select(\"content_model\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 25.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "548219"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df.filter(df[\"content_model\"].isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 25.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "548219"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df.filter(df[\"content_model\"].isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It seems like every other row in this dataset consists of just nulls,\n",
    "# so I remove those rows\n",
    "df_not_null = df.filter(df[\"content_model\"].isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|language|\n",
      "+--------+\n",
      "|      nb|\n",
      "+--------+\n",
      "\n",
      "Wall time: 26.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# After removing the nulls, there is only one language (where previously null also showed up)\n",
    "df_not_null.select(\"language\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|        0|\n",
      "+---------+\n",
      "\n",
      "Wall time: 25.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# About namespaces https://en.wikipedia.org/wiki/Wikipedia:Namespace\n",
    "# Articles have no namespace (no prefix), we are interested in namespace 0.\n",
    "# It seems this dataset only contains the namespace we are interested in!\n",
    "df_not_null.select(\"namespace\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out columns\n",
    "filtered_df = df_not_null.drop(\"content_model\", \"language\", \"category\", \"coordinates\", \"defaultsort\", \\\n",
    "        \"external_link\", \"heading\", \"incoming_links\", \"namespace\", \"namespace_text\", \\\n",
    "        \"outgoing_link\", \"redirect\", \"text_bytes\", \"template\", \"wiki\", \\\n",
    "        \"wikibase_item\", \"version_type\", \"file_bits\", \"file_height\", \"file_media_type\", \\\n",
    "        \"file_resolution\", \"file_size\", \"file_text\", \"file_width\", \"index\", \\\n",
    "        \"file_mime\", \"ores_articletopic\", \"ores_articletopics\", \"score\", \"popularity_score\", \\\n",
    "        \"display_title\", \"auxiliary_text\", \"create_timestamp\", \"opening_text\", \"source_text\", \\\n",
    "        \"timestamp\", \"version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|                text|     title|\n",
      "+--------------------+----------+\n",
      "|Kill Buljo er en ...|Kill Buljo|\n",
      "+--------------------+----------+\n",
      "\n",
      "Wall time: 25.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Seems like only one entry per article. That's good! I feared it was one per revision.\n",
    "filtered_df.filter(filtered_df[\"title\"] == \"Kill Buljo\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 25 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Kill Buljo er en norsk film fra 2008. Den er en komisk parodi på det amerikanske actioneeventyret Kill Bill, og handlingen er lagt til Finnmark. Regi er ved Tommy Wirkola og manus Stig Frode Henriksen og Tommy Wirkola. Filmen hadde premiere i mars 2007, og ble sett av 87 000 på kino i Norge. Det ble solgt over 95 000 DVD-er. Filmmusikken ble bl.a. laget av Alta-bandet Cyaneed. Kill Buljo 2 kom ut i 2013. Jompa Tormann er rollefiguren som spilles av Stig Frode Henriksen i filmen Kill Buljo. Rollefiguren dukker også opp i flere kortfilmer på DVD-en Kill Buljo: The Beginning. Jompa Tormann: Stig Frode Henriksen Pappa Buljo: Frank Arne Olsen Tampa Buljo: Martin Hykkerud Sid Wisløff: Tommy Wirkola Unni Formen: Natasha Angel Dahle Peggy Mathilassi: Linda Øverlie Nilsen Crazy Beibifeit: Ørjan Gamst Kjell Driver: John Even Pedersen Bud Light: Christian Reiertsen Lara Kofta: Merete Nordahl Mr. Handjagi: Ørjan Gamst Kato: Jørn Tore Nilsen Blow Job: Heidi Monsen Troll Tove: Eirik Junge Eliassen Sid Wisløffs far: Kristian Figenschow Mona Smurfen: Aina Timbiani Anmeldelse i Verdens Gang Anmeldelse i Bergens Tidende Anmeldelse i Nettavisen Anmeldelse i Stavanger Aftenblad Anmeldelse i Adresseavisen ^ Endring av aldersgrense på filmen Kill Buljo – the movie[død lenke], Medietilsynet, 29. mars 2007 ^ Bergensavisen – Mer «Kill Buljo» Offisielt nettsted (en) Kill Buljo på Internet Movie Database (no) Kill Buljo i Nasjonalbibliotekets filmografi (en) Kill Buljo på AllMovie (en) Kill Buljo på Turner Classic Movies (en) Kill Buljo på Rotten Tomatoes Dagbladet – Kill Buljo til utlandet Kill Buljo hos Filmweb Denne norske filmrelaterte artikkelen er foreløpig kort eller mangelfull, og du kan hjelpe Wikipedia ved å utvide den. Det finnes mer utfyllende artikkel/artikler på \\xa0.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Comparing the content in the text column to the content on the web page\n",
    "# https://no.wikipedia.org/wiki/Kill_Buljo\n",
    "filtered_df.filter(filtered_df[\"title\"] == \"Kill Buljo\").select(\"text\").collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the text column contains everything in the article, so I went back and removed all columns except `text` and `title`.\n",
    "\n",
    "The text content itself could need some cleaning. There are several instances of `(en)` which looking at the web page seems to be the language of the external reference, so at least remove `(en)` and `(no)`. At one point we see `[død lenke]` which is an inline footnote, possibly a generic one. Going to the web page we find that it was created by a bot. On Wikipedia we find a web page listing [all referance template tags that can appear in running text](https://no.wikipedia.org/wiki/Mal:Trenger_referanse).\n",
    "\n",
    "Also found a [list over sentences called \"stub\"](https://no.wikipedia.org/wiki/Kategori:Stubbmaler) that can appear in the running text. Unfortunately there were 361 of them, and much more template [on the list of all tempaltes](https://no.wikipedia.org/wiki/Kategori:Maler). I tried to use the other dataset (`...general.json`) to find the text the templates produces so I could remove that, but gave up.\n",
    "\n",
    "We see that \\xa0 appears.\n",
    "\n",
    "And lastly we want to remove punctuation marks, digits and other special characters. Basically be left with only words consisting of letters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the text content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_template_reference_tags = [\"[trenger referanse]\", \"[klargjør]\", \"[hvor?]\", \"[trenger oppdatering]\", \"[død lenke]\", \"[trenger sitat]\", \"[trenger bedre kilde]\", \"[bør utdypes]\", \"[hvem?]\", \"[omstridt – diskuter]\", \"[ufullstendig referanse]\", \"[ikke i angitt kilde]\", \"[tredjepartskilde trengs]\", \"[når?]\", \"[av hvem?]\", \"[sic]\", \"[trenger sidetall]\"]\n",
    "\n",
    "# First time I understand this is a generator and not just a for loop\n",
    "# Weirdly enough, the ISO 639-3 alpha_2 codes doesn't contain \"en\"\n",
    "ext_ref_language_tags = [f\"({country.alpha_2.lower()})\" for country in pycountry.countries] + [\"(en)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_possible_template_text = []\n",
    "# It is empty because I don't know! I tried to find the templates in \n",
    "# \"Mining meta information about Wikipedia.ipynb\", but failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use RDD features to map and reduce\n",
    "# Transform from Row with title and text to just text strings, rigth away\n",
    "rdd = filtered_df.rdd.map(lambda row: row.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_strings(src_string: str, strings_to_remove: list):\n",
    "    new_string= src_string\n",
    "    for s in strings_to_remove:\n",
    "        new_string = new_string.replace(s, \"\")\n",
    "    return new_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hundene i Riga er en svensk film fra 1995 av Per Berglund med Rolf Lassgård, Björn Kjellman, Charlotte Sieling og Paul Butkevich i noen av rollene. Filmen basert seg på romanen Hundene i Riga av Henning Mankell som er den andre i serien om Kurt Wallander. En livbåt flyter i land ved den skånske kysten. I båten befinner det seg to menn som har blitt myrdet. Etterforsker Kurt Wallander fra politiet i Ystad tilkalles til plassen. Ved hjelp av politiet i Latvia blir begge mennene identifisert. For å lette utredningen ble en politimann tilkalt fra Latvia for å hjelpe til å løse saken. Men når politimannen vender tilbake til Latvia blir han myrdet. Kurt Wallander flyr til Latvia for å forsøke å finne ut hvorfor politimannen ble myrdet. (en) Hundene i Riga på Internet Movie Database (sv) Hundene i Riga i Svensk Filmdatabas (en) Hundene i Riga på Rotten Tomatoes Portal: Film',\n",
       " 'Kill Buljo er en norsk film fra 2008. Den er en komisk parodi på det amerikanske actioneeventyret Kill Bill, og handlingen er lagt til Finnmark. Regi er ved Tommy Wirkola og manus Stig Frode Henriksen og Tommy Wirkola. Filmen hadde premiere i mars 2007, og ble sett av 87 000 på kino i Norge. Det ble solgt over 95 000 DVD-er. Filmmusikken ble bl.a. laget av Alta-bandet Cyaneed. Kill Buljo 2 kom ut i 2013. Jompa Tormann er rollefiguren som spilles av Stig Frode Henriksen i filmen Kill Buljo. Rollefiguren dukker også opp i flere kortfilmer på DVD-en Kill Buljo: The Beginning. Jompa Tormann: Stig Frode Henriksen Pappa Buljo: Frank Arne Olsen Tampa Buljo: Martin Hykkerud Sid Wisløff: Tommy Wirkola Unni Formen: Natasha Angel Dahle Peggy Mathilassi: Linda Øverlie Nilsen Crazy Beibifeit: Ørjan Gamst Kjell Driver: John Even Pedersen Bud Light: Christian Reiertsen Lara Kofta: Merete Nordahl Mr. Handjagi: Ørjan Gamst Kato: Jørn Tore Nilsen Blow Job: Heidi Monsen Troll Tove: Eirik Junge Eliassen Sid Wisløffs far: Kristian Figenschow Mona Smurfen: Aina Timbiani Anmeldelse i Verdens Gang Anmeldelse i Bergens Tidende Anmeldelse i Nettavisen Anmeldelse i Stavanger Aftenblad Anmeldelse i Adresseavisen ^ Endring av aldersgrense på filmen Kill Buljo – the movie, Medietilsynet, 29. mars 2007 ^ Bergensavisen – Mer «Kill Buljo» Offisielt nettsted (en) Kill Buljo på Internet Movie Database (no) Kill Buljo i Nasjonalbibliotekets filmografi (en) Kill Buljo på AllMovie (en) Kill Buljo på Turner Classic Movies (en) Kill Buljo på Rotten Tomatoes Dagbladet – Kill Buljo til utlandet Kill Buljo hos Filmweb Denne norske filmrelaterte artikkelen er foreløpig kort eller mangelfull, og du kan hjelpe Wikipedia ved å utvide den. Det finnes mer utfyllende artikkel/artikler på \\xa0.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove possible_template_reference_tags\n",
    "removed_templates_rdd = rdd.map(lambda s: remove_strings(s, possible_template_reference_tags))\n",
    "removed_templates_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hundene i Riga er en svensk film fra 1995 av Per Berglund med Rolf Lassgård, Björn Kjellman, Charlotte Sieling og Paul Butkevich i noen av rollene. Filmen basert seg på romanen Hundene i Riga av Henning Mankell som er den andre i serien om Kurt Wallander. En livbåt flyter i land ved den skånske kysten. I båten befinner det seg to menn som har blitt myrdet. Etterforsker Kurt Wallander fra politiet i Ystad tilkalles til plassen. Ved hjelp av politiet i Latvia blir begge mennene identifisert. For å lette utredningen ble en politimann tilkalt fra Latvia for å hjelpe til å løse saken. Men når politimannen vender tilbake til Latvia blir han myrdet. Kurt Wallander flyr til Latvia for å forsøke å finne ut hvorfor politimannen ble myrdet.  Hundene i Riga på Internet Movie Database  Hundene i Riga i Svensk Filmdatabas  Hundene i Riga på Rotten Tomatoes Portal: Film',\n",
       " 'Kill Buljo er en norsk film fra 2008. Den er en komisk parodi på det amerikanske actioneeventyret Kill Bill, og handlingen er lagt til Finnmark. Regi er ved Tommy Wirkola og manus Stig Frode Henriksen og Tommy Wirkola. Filmen hadde premiere i mars 2007, og ble sett av 87 000 på kino i Norge. Det ble solgt over 95 000 DVD-er. Filmmusikken ble bl.a. laget av Alta-bandet Cyaneed. Kill Buljo 2 kom ut i 2013. Jompa Tormann er rollefiguren som spilles av Stig Frode Henriksen i filmen Kill Buljo. Rollefiguren dukker også opp i flere kortfilmer på DVD-en Kill Buljo: The Beginning. Jompa Tormann: Stig Frode Henriksen Pappa Buljo: Frank Arne Olsen Tampa Buljo: Martin Hykkerud Sid Wisløff: Tommy Wirkola Unni Formen: Natasha Angel Dahle Peggy Mathilassi: Linda Øverlie Nilsen Crazy Beibifeit: Ørjan Gamst Kjell Driver: John Even Pedersen Bud Light: Christian Reiertsen Lara Kofta: Merete Nordahl Mr. Handjagi: Ørjan Gamst Kato: Jørn Tore Nilsen Blow Job: Heidi Monsen Troll Tove: Eirik Junge Eliassen Sid Wisløffs far: Kristian Figenschow Mona Smurfen: Aina Timbiani Anmeldelse i Verdens Gang Anmeldelse i Bergens Tidende Anmeldelse i Nettavisen Anmeldelse i Stavanger Aftenblad Anmeldelse i Adresseavisen ^ Endring av aldersgrense på filmen Kill Buljo – the movie, Medietilsynet, 29. mars 2007 ^ Bergensavisen – Mer «Kill Buljo» Offisielt nettsted  Kill Buljo på Internet Movie Database  Kill Buljo i Nasjonalbibliotekets filmografi  Kill Buljo på AllMovie  Kill Buljo på Turner Classic Movies  Kill Buljo på Rotten Tomatoes Dagbladet – Kill Buljo til utlandet Kill Buljo hos Filmweb Denne norske filmrelaterte artikkelen er foreløpig kort eller mangelfull, og du kan hjelpe Wikipedia ved å utvide den. Det finnes mer utfyllende artikkel/artikler på \\xa0.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove language tags\n",
    "removed_lang_tags_rdd = removed_templates_rdd.map(lambda s: remove_strings(s, ext_ref_language_tags))\n",
    "removed_lang_tags_rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatisation\n",
    "\n",
    "I purposefully didn't [stem](https://en.wikipedia.org/wiki/Stemming) the words, since that could lead to non-existing words. E.g. `opparbeide` -> the stem `opparbeid`, `adresse` -> `adress`. The first time I got to the bottom of this notebook I found that the result contained different [inflection](https://en.wikipedia.org/wiki/Inflection) of the word \"artikkel\", \"artikkelene\", \"artikler\". Although I didn't want to stem the words, I still wanted to avoid having different inflections of the same root word.\n",
    "\n",
    "After some Googling I learned I was looking for the [lemma](https://en.wikipedia.org/wiki/Lemma_(morphology)), and finding it is called lemmatisation.\n",
    "\n",
    "I just decided to go with the package spaCy since had a simple API and does [POS tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging) automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run the following line once. The leading exclamation mark means it is a terminal command\n",
    "# !python -m spacy download nb_core_news_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got problems with memory tryng to call the above function with PySpark (using map). You can read about some problems [in this blog post](https://haridas.in/run-spacy-jobs-on-apache-spark.html). After some trying and failing I decided to simply run spaCy with plain Python. \n",
    "\n",
    "Since I don't have enough RAM to load all data and don't think it is possibly to lazily iterate over an RDD, I must dump data to storage at this point. Note, if you get memory error while running `saveAsTextFile` method on the RDD on Windows, [see this Stack Overflow answer](https://stackoverflow.com/questions/40764807/null-entry-in-command-string-exception-in-saveastextfile-on-pyspark/40958969). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o101.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/E:/workspace3/online-alias/words/temp1 already exists\r\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\r\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:289)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda$3167/000000000000000000.apply$mcV$sp(Unknown Source)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda$3165/000000000000000000.apply$mcV$sp(Unknown Source)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda$3164/000000000000000000.apply$mcV$sp(Unknown Source)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda$3163/000000000000000000.apply$mcV$sp(Unknown Source)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1552)\r\n\tat org.apache.spark.rdd.RDD$$Lambda$3161/000000000000000000.apply$mcV$sp(Unknown Source)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1552)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1538)\r\n\tat org.apache.spark.rdd.RDD$$Lambda$3160/000000000000000000.apply$mcV$sp(Unknown Source)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1538)\r\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:550)\r\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:549)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:836)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32me:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36msaveAsTextFile\u001b[1;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[0;32m   1654\u001b[0m             \u001b[0mkeyed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompressionCodec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1655\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1656\u001b[1;33m             \u001b[0mkeyed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1657\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1658\u001b[0m     \u001b[1;31m# Pair functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o101.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/E:/workspace3/online-alias/words/temp1 already exists\r\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\r\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:289)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda$3167/000000000000000000.apply$mcV$sp(Unknown Source)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda$3165/000000000000000000.apply$mcV$sp(Unknown Source)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda$3164/000000000000000000.apply$mcV$sp(Unknown Source)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda$3163/000000000000000000.apply$mcV$sp(Unknown Source)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1552)\r\n\tat org.apache.spark.rdd.RDD$$Lambda$3161/000000000000000000.apply$mcV$sp(Unknown Source)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1552)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1538)\r\n\tat org.apache.spark.rdd.RDD$$Lambda$3160/000000000000000000.apply$mcV$sp(Unknown Source)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1538)\r\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:550)\r\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:549)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:836)\r\n"
     ]
    }
   ],
   "source": [
    "# Uncomment to dump\n",
    "# %%time\n",
    "# removed_lang_tags_rdd.saveAsTextFile(\"temp1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run lemmatisation in batches and write to file\n",
    "\n",
    "If you already have run the cell above once, you can start from this cell later!\n",
    "\n",
    "> Remember to run the first cell with the imports, though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nb_core_news_lg\n",
    "nlp = nb_core_news_lg.load()\n",
    "\n",
    "def lemmatize_documents(src_documents: list):\n",
    "    docs = nlp.pipe(texts=src_documents)\n",
    "    new_documents = []\n",
    "    for doc in docs:\n",
    "        new_string = \"\"\n",
    "        for token in doc:\n",
    "            if (token.lemma_ != \"-PRON-\"):\n",
    "                new_string += f\" {token.lemma_}\"\n",
    "            else:\n",
    "                # spacy returns the lemma \"-PRON-\" for pronouns, and we don't want that,\n",
    "                # in that case we return the original word istead\n",
    "                new_string += f\" {token}\"\n",
    "        new_documents.append(new_string)\n",
    "    return new_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START processing part-00000\n",
      "END processing part-00000. It had 13109 lines. It took 668 seconds\n",
      "START processing part-00001\n",
      "END processing part-00001. It had 12927 lines. It took 582 seconds\n",
      "START processing part-00002\n",
      "END processing part-00002. It had 13149 lines. It took 649 seconds\n",
      "START processing part-00003\n",
      "END processing part-00003. It had 13598 lines. It took 554 seconds\n",
      "START processing part-00004\n",
      "END processing part-00004. It had 13428 lines. It took 554 seconds\n",
      "START processing part-00005\n",
      "END processing part-00005. It had 13619 lines. It took 571 seconds\n",
      "START processing part-00006\n",
      "END processing part-00006. It had 13054 lines. It took 602 seconds\n",
      "START processing part-00007\n",
      "END processing part-00007. It had 13396 lines. It took 581 seconds\n",
      "START processing part-00008\n",
      "END processing part-00008. It had 13326 lines. It took 594 seconds\n",
      "START processing part-00009\n",
      "END processing part-00009. It had 13163 lines. It took 586 seconds\n",
      "START processing part-00010\n",
      "END processing part-00010. It had 13074 lines. It took 580 seconds\n",
      "START processing part-00011\n",
      "END processing part-00011. It had 12947 lines. It took 597 seconds\n",
      "START processing part-00012\n",
      "END processing part-00012. It had 12877 lines. It took 602 seconds\n",
      "START processing part-00013\n",
      "END processing part-00013. It had 13117 lines. It took 593 seconds\n",
      "START processing part-00014\n",
      "END processing part-00014. It had 13343 lines. It took 568 seconds\n",
      "START processing part-00015\n",
      "END processing part-00015. It had 12782 lines. It took 614 seconds\n",
      "START processing part-00016\n",
      "END processing part-00016. It had 13346 lines. It took 580 seconds\n",
      "START processing part-00017\n",
      "END processing part-00017. It had 13102 lines. It took 598 seconds\n",
      "START processing part-00018\n",
      "END processing part-00018. It had 12992 lines. It took 604 seconds\n",
      "START processing part-00019\n",
      "END processing part-00019. It had 12936 lines. It took 595 seconds\n",
      "START processing part-00020\n",
      "END processing part-00020. It had 12796 lines. It took 591 seconds\n",
      "START processing part-00021\n",
      "END processing part-00021. It had 12843 lines. It took 590 seconds\n",
      "START processing part-00022\n",
      "END processing part-00022. It had 12907 lines. It took 600 seconds\n",
      "START processing part-00023\n",
      "END processing part-00023. It had 12750 lines. It took 602 seconds\n",
      "START processing part-00024\n",
      "END processing part-00024. It had 12807 lines. It took 602 seconds\n",
      "START processing part-00025\n",
      "END processing part-00025. It had 12765 lines. It took 608 seconds\n",
      "START processing part-00026\n",
      "END processing part-00026. It had 12736 lines. It took 610 seconds\n",
      "START processing part-00027\n",
      "END processing part-00027. It had 12929 lines. It took 607 seconds\n",
      "START processing part-00028\n",
      "END processing part-00028. It had 12755 lines. It took 621 seconds\n",
      "START processing part-00029\n",
      "END processing part-00029. It had 12798 lines. It took 605 seconds\n",
      "START processing part-00030\n",
      "END processing part-00030. It had 11578 lines. It took 808 seconds\n",
      "START processing part-00031\n",
      "9000 lines processed\r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(\"./out/lemmatised_wikipedia.txt\", \"a\", encoding=\"utf-8\") as f_out:\n",
    "    temp_folder = \"./temp1\"\n",
    "    directory = os.fsencode(temp_folder)\n",
    "    \n",
    "    for file in os.listdir(directory):\n",
    "        starttime = time.time()\n",
    "        filename = os.fsdecode(file)\n",
    "        if (filename.endswith(\".crc\") or filename == \"_SUCCESS\"):\n",
    "            continue\n",
    "        f_in = open(f\"{temp_folder}/{filename}\", encoding=\"utf-8\")\n",
    "        print(f\"START processing {filename}\")\n",
    "        # Read file in parts to avoid yet another memory problem:\n",
    "        # MemoryError: Unable to allocate 2.08 GiB for an array with shape (546345, 1024) and data type float32\n",
    "        # If there are too many lines in combinations with large articles, I don't have enough RAM (8GB)\n",
    "        docs = []\n",
    "        num_lines = 0\n",
    "        print(\"0 lines processed\", end='\\r')\n",
    "        batch_size = 1000\n",
    "        if (\"part-00032\" in filename):\n",
    "            batch_size = 250\n",
    "        for line in f_in:\n",
    "            docs.append(line)\n",
    "            if (len(docs) == batch_size):\n",
    "                try:\n",
    "                    lemmatised_docs = lemmatize_documents(docs)\n",
    "                    f_out.writelines(lemmatised_docs)\n",
    "                    num_lines += batch_size\n",
    "                    docs = []\n",
    "                except MemoryError:\n",
    "                    print(f\"Error happened between lines {num_lines} and {num_lines + batch_size}\")\n",
    "                    raise\n",
    "                print(f\"{num_lines} lines processed\", end='\\r')\n",
    "        if (len(docs) > 0):\n",
    "            lemmatised_docs = lemmatize_documents(docs)\n",
    "            f_out.writelines(lemmatised_docs)\n",
    "            print(f\"{num_lines} lines processed\", end='\\r')\n",
    "            num_lines += len(docs)\n",
    "        f_in.close()\n",
    "        print(f\"END processing {filename}. It had {num_lines} lines. It took {round(time.time() - starttime)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_letter_words_and_chars(src_string: str):\n",
    "    whitelist = set('abcdefghijklmnopqrstuvwxyzæøå ABCDEFGHIJKLMNOPQRSTUVWXYZÆØÅ')\n",
    "    new_string = \"\"\n",
    "    for char in src_string:\n",
    "        if char in whitelist:\n",
    "            new_string = new_string + char\n",
    "        else:\n",
    "            new_string = new_string + \" \"\n",
    "    return new_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lemmatized_rdd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-d523e0713766>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0monly_letters_rdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlemmatized_rdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mremove_non_letter_words_and_chars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0monly_letters_rdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lemmatized_rdd' is not defined"
     ]
    }
   ],
   "source": [
    "only_letters_rdd = lemmatized_rdd.map(lambda s: remove_non_letter_words_and_chars(s))\n",
    "only_letters_rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_lowercase_if_not_acronym(src_string: str):\n",
    "    for letter in src_string:\n",
    "        if letter.islower():\n",
    "            return src_string.lower()\n",
    "    return src_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.42 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hund',\n",
       " 'riga',\n",
       " 'er',\n",
       " 'en',\n",
       " 'svensk',\n",
       " 'film',\n",
       " 'fra',\n",
       " 'av',\n",
       " 'per',\n",
       " 'berglund',\n",
       " 'med',\n",
       " 'rolf',\n",
       " 'lassgård',\n",
       " 'bj',\n",
       " 'rn',\n",
       " 'kjellman',\n",
       " 'charlotte',\n",
       " 'sieling',\n",
       " 'og',\n",
       " 'paul']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Convert string to list of lowercase words larger with two or more characters\n",
    "words_rdd = only_letters_rdd \\\n",
    "    .flatMap(lambda s: s.split(\" \")) \\\n",
    "    .filter(lambda word: len(word) > 1) \\\n",
    "    .map(lambda word: convert_to_lowercase_if_not_acronym(word))\n",
    "words_rdd.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['og', 'i', 'jeg', 'det', 'at', 'en', 'et', 'den', 'til', 'er']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of stopwords taken from http://snowball.tartarus.org/algorithms/norwegian/stop.txt\n",
    "stopwords = []\n",
    "with open(\"./norwegian_stopwords.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        strings = line.split(\"|\")\n",
    "        potential_stopword = strings[0].strip()\n",
    "        if (not potential_stopword == \"\"):\n",
    "            stopwords.append(potential_stopword)\n",
    "\n",
    "# Remove English stopwords as well since I noticed \"the\", \"in\", and \"of\" in the top 20\n",
    "# Inspired by the Google History stopword list found at https://www.ranks.nl/stopwords\n",
    "english_stopwords = [\"I\", \"a\", \"an\", \"are\", \"as\", \"at\", \"by\", \"com\", \"for\", \"from\", \"how\", \"in\", \"it\", \"of\", \"on\", \"that\", \"the\", \"this\", \"was\", \"what\", \"when\", \"where\", \"who\", \"will\", \"with\", \"www\"]\n",
    "stopwords = stopwords + english_stopwords\n",
    "stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_word_in_list(src_word: str, list_of_words: list):\n",
    "    if (src_word in list_of_words):\n",
    "        return \"\"\n",
    "    else:\n",
    "        return src_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.43 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hund',\n",
       " 'riga',\n",
       " 'svensk',\n",
       " 'film',\n",
       " 'per',\n",
       " 'berglund',\n",
       " 'rolf',\n",
       " 'lassgård',\n",
       " 'bj',\n",
       " 'rn',\n",
       " 'kjellman',\n",
       " 'charlotte',\n",
       " 'sieling',\n",
       " 'paul',\n",
       " 'butkevich',\n",
       " 'rolle',\n",
       " 'film',\n",
       " 'basere',\n",
       " 'roman',\n",
       " 'hund']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Remove all stop-words\n",
    "no_stopwords_rdd = words_rdd \\\n",
    "    .map(lambda word: remove_word_in_list(word, stopwords)) \\\n",
    "    .filter(lambda word: word != \"\")\n",
    "no_stopwords_rdd.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mining!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 57.0 failed 1 times, most recent failure: Lost task 3.0 in stage 57.0 (TID 982, Stian-PC.mshome.net, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 605, in main\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 595, in process\n  File \"e:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\pyspark\\rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"e:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\pyspark\\rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"e:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\pyspark\\rdd.py\", line 425, in func\n    return f(iterator)\n  File \"e:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\pyspark\\rdd.py\", line 1946, in combineLocally\n    merger.mergeValues(iterator)\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-38-783a6dc03d6f>\", line 1, in <lambda>\n  File \"<ipython-input-36-61959dd0e36b>\", line 4, in lemmatize_string\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\nb_core_news_lg\\__init__.py\", line 12, in load\n    return load_model_from_init_py(__file__, **overrides)\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\spacy\\util.py\", line 239, in load_model_from_init_py\n    return load_model_from_path(data_path, meta, **overrides)\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\spacy\\util.py\", line 222, in load_model_from_path\n    return nlp.from_disk(model_path, exclude=disable)\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\spacy\\language.py\", line 974, in from_disk\n    util.from_disk(path, deserializers, exclude)\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\spacy\\util.py\", line 690, in from_disk\n    reader(path / key)\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\spacy\\language.py\", line 950, in deserialize_vocab\n    self.vocab.from_disk(path)\n  File \"vocab.pyx\", line 475, in spacy.vocab.Vocab.from_disk\n  File \"vectors.pyx\", line 432, in spacy.vectors.Vectors.from_disk\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\spacy\\util.py\", line 690, in from_disk\n    reader(path / key)\n  File \"vectors.pyx\", line 425, in spacy.vectors.Vectors.from_disk.load_vectors\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\numpy\\lib\\npyio.py\", line 439, in load\n    return format.read_array(fid, allow_pickle=allow_pickle,\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\numpy\\lib\\format.py\", line 741, in read_array\n    array = numpy.fromfile(fp, dtype=dtype, count=count)\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 572. MiB for an array with shape (150000000,) and data type float32\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2119/000000000000000000.apply(Unknown Source)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:836)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$3173/000000000000000000.apply(Unknown Source)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$3171/000000000000000000.apply(Unknown Source)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2164)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\r\n\tat org.apache.spark.rdd.RDD$$Lambda$1884/000000000000000000.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:168)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:836)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 605, in main\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 595, in process\n  File \"e:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\pyspark\\rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"e:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\pyspark\\rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"e:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\pyspark\\rdd.py\", line 425, in func\n    return f(iterator)\n  File \"e:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\pyspark\\rdd.py\", line 1946, in combineLocally\n    merger.mergeValues(iterator)\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-38-783a6dc03d6f>\", line 1, in <lambda>\n  File \"<ipython-input-36-61959dd0e36b>\", line 4, in lemmatize_string\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\nb_core_news_lg\\__init__.py\", line 12, in load\n    return load_model_from_init_py(__file__, **overrides)\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\spacy\\util.py\", line 239, in load_model_from_init_py\n    return load_model_from_path(data_path, meta, **overrides)\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\spacy\\util.py\", line 222, in load_model_from_path\n    return nlp.from_disk(model_path, exclude=disable)\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\spacy\\language.py\", line 974, in from_disk\n    util.from_disk(path, deserializers, exclude)\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\spacy\\util.py\", line 690, in from_disk\n    reader(path / key)\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\spacy\\language.py\", line 950, in deserialize_vocab\n    self.vocab.from_disk(path)\n  File \"vocab.pyx\", line 475, in spacy.vocab.Vocab.from_disk\n  File \"vectors.pyx\", line 432, in spacy.vectors.Vectors.from_disk\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\spacy\\util.py\", line 690, in from_disk\n    reader(path / key)\n  File \"vectors.pyx\", line 425, in spacy.vectors.Vectors.from_disk.load_vectors\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\numpy\\lib\\npyio.py\", line 439, in load\n    return format.read_array(fid, allow_pickle=allow_pickle,\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\numpy\\lib\\format.py\", line 741, in read_array\n    array = numpy.fromfile(fp, dtype=dtype, count=count)\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 572. MiB for an array with shape (150000000,) and data type float32\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2119/000000000000000000.apply(Unknown Source)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32me:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36msortBy\u001b[1;34m(self, keyfunc, ascending, numPartitions)\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'b'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'd'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    771\u001b[0m         \"\"\"\n\u001b[1;32m--> 772\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeyBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msortByKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumPartitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    774\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mglom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36msortByKey\u001b[1;34m(self, ascending, numPartitions, keyfunc)\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;31m# the key-space into bins such that the bins have roughly the same\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m         \u001b[1;31m# number of (key, value) pairs falling into them\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 740\u001b[1;33m         \u001b[0mrddSize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    741\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mrddSize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m  \u001b[1;31m# empty RDD\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcount\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1139\u001b[0m         \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1140\u001b[0m         \"\"\"\n\u001b[1;32m-> 1141\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1130\u001b[0m         \u001b[1;36m6.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \"\"\"\n\u001b[1;32m-> 1132\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mfold\u001b[1;34m(self, zeroValue, op)\u001b[0m\n\u001b[0;32m   1001\u001b[0m         \u001b[1;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1002\u001b[0m         \u001b[1;31m# to the final reduce call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1003\u001b[1;33m         \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1004\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    887\u001b[0m         \"\"\"\n\u001b[0;32m    888\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 57.0 failed 1 times, most recent failure: Lost task 3.0 in stage 57.0 (TID 982, Stian-PC.mshome.net, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 605, in main\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 595, in process\n  File \"e:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\pyspark\\rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"e:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\pyspark\\rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"e:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\pyspark\\rdd.py\", line 425, in func\n    return f(iterator)\n  File \"e:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\pyspark\\rdd.py\", line 1946, in combineLocally\n    merger.mergeValues(iterator)\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-38-783a6dc03d6f>\", line 1, in <lambda>\n  File \"<ipython-input-36-61959dd0e36b>\", line 4, in lemmatize_string\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\nb_core_news_lg\\__init__.py\", line 12, in load\n    return load_model_from_init_py(__file__, **overrides)\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\spacy\\util.py\", line 239, in load_model_from_init_py\n    return load_model_from_path(data_path, meta, **overrides)\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\spacy\\util.py\", line 222, in load_model_from_path\n    return nlp.from_disk(model_path, exclude=disable)\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\spacy\\language.py\", line 974, in from_disk\n    util.from_disk(path, deserializers, exclude)\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\spacy\\util.py\", line 690, in from_disk\n    reader(path / key)\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\spacy\\language.py\", line 950, in deserialize_vocab\n    self.vocab.from_disk(path)\n  File \"vocab.pyx\", line 475, in spacy.vocab.Vocab.from_disk\n  File \"vectors.pyx\", line 432, in spacy.vectors.Vectors.from_disk\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\spacy\\util.py\", line 690, in from_disk\n    reader(path / key)\n  File \"vectors.pyx\", line 425, in spacy.vectors.Vectors.from_disk.load_vectors\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\numpy\\lib\\npyio.py\", line 439, in load\n    return format.read_array(fid, allow_pickle=allow_pickle,\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\numpy\\lib\\format.py\", line 741, in read_array\n    array = numpy.fromfile(fp, dtype=dtype, count=count)\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 572. MiB for an array with shape (150000000,) and data type float32\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2119/000000000000000000.apply(Unknown Source)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:836)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$3173/000000000000000000.apply(Unknown Source)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$3171/000000000000000000.apply(Unknown Source)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2164)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\r\n\tat org.apache.spark.rdd.RDD$$Lambda$1884/000000000000000000.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:168)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:836)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 605, in main\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 595, in process\n  File \"e:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\pyspark\\rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"e:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\pyspark\\rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"e:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\pyspark\\rdd.py\", line 425, in func\n    return f(iterator)\n  File \"e:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\pyspark\\rdd.py\", line 1946, in combineLocally\n    merger.mergeValues(iterator)\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-38-783a6dc03d6f>\", line 1, in <lambda>\n  File \"<ipython-input-36-61959dd0e36b>\", line 4, in lemmatize_string\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\nb_core_news_lg\\__init__.py\", line 12, in load\n    return load_model_from_init_py(__file__, **overrides)\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\spacy\\util.py\", line 239, in load_model_from_init_py\n    return load_model_from_path(data_path, meta, **overrides)\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\spacy\\util.py\", line 222, in load_model_from_path\n    return nlp.from_disk(model_path, exclude=disable)\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\spacy\\language.py\", line 974, in from_disk\n    util.from_disk(path, deserializers, exclude)\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\spacy\\util.py\", line 690, in from_disk\n    reader(path / key)\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\spacy\\language.py\", line 950, in deserialize_vocab\n    self.vocab.from_disk(path)\n  File \"vocab.pyx\", line 475, in spacy.vocab.Vocab.from_disk\n  File \"vectors.pyx\", line 432, in spacy.vectors.Vectors.from_disk\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\spacy\\util.py\", line 690, in from_disk\n    reader(path / key)\n  File \"vectors.pyx\", line 425, in spacy.vectors.Vectors.from_disk.load_vectors\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\numpy\\lib\\npyio.py\", line 439, in load\n    return format.read_array(fid, allow_pickle=allow_pickle,\n  File \"E:\\workspace3\\online-alias\\words\\.venv\\lib\\site-packages\\numpy\\lib\\format.py\", line 741, in read_array\n    array = numpy.fromfile(fp, dtype=dtype, count=count)\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 572. MiB for an array with shape (150000000,) and data type float32\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2119/000000000000000000.apply(Unknown Source)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Finding the most popular words\n",
    "\n",
    "ranked_words_rdd = no_stopwords_rdd.map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(add) \\\n",
    "    .sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "endTime = time.time()\n",
    "ranked_words_rdd.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_words_list = ranked_words_rdd.map(lambda t: t[0]).take(1000)\n",
    "# with open(\"./out/wikipedia_words.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     for word in final_words_list:\n",
    "#         f.write(f\"{word}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
